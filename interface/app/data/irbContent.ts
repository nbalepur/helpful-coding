export const irbConsentContent = `## Project Title

**Aligning Machine Learning Models to Personalized and Downstream User Needs**

## Purpose of the Study

This research is being conducted by Jordan Boyd-Graber at the University of Maryland, College Park. We are inviting you to participate in this research project because you are interested in and often perform writing, coding, information retrieval, and decision-making tasks. The purpose of this research project is to use machine learning-based advice and instructions to make performing these tasks more efficient and accurate.

## Procedures

You will participate in tasks designed to see how model-generated instructions and advice affect your ability to complete tasks accurately and efficiently. The tasks will fall into four categories: coding, writing, decision-making, and information seeking (e.g., finding documents or answering questions). You'll begin by choosing one of these categories, then receive specific tasks based on your choice. For example, in the coding category, you might be asked to write a function, or in decision-making, to classify an email as spam. These tasks come from existing datasets in natural language processing and machine learning.

You'll be placed in one of two groups: 1) completing tasks on your own, or 2) completing tasks with model-generated instructions. This allows us to measure how the model's advice affects your performance in terms of accuracy and speed. We'll measure how accurately and quickly you complete each one using automated metrics, such as test case completion for coding tasks or grammar and coherence for writing. You can also provide preferences (e.g., "I prefer code with no comments") to personalize the model's instructions. This helps us see how personalized advice impacts your performance.

Sessions in group (2) will also be asked to perform "skill checks" after each session, where they will complete simpler versions of the AI-assistance tasks but without AI assistance (e.g. educational assessments, implementing a simple website/Leetcode-style function without assistance versus a complex framework with AI assistance). This will help us assess how your use of AI impacts your abilities without it.

Some sessions will include competitive tasks where you'll compete against others. These will follow the same procedure (with or without model-generated instructions), but we'll also measure your performance against others. Such competitions may also include subjective voting and expert review, where you can vote on others' performance and receive votes on your own.

Top performers, measured through task completion rate (accuracy) and efficiency (time taken) in non-competition studies and additionally user voting or expert review in the competition studies may earn rewards in both the competition and non-competition based user studies, explained below.

Each session will last about 60 minutes, with one task per minute. Sessions will run from December 2025 to December 2026, and you may be invited to join additional sessions, though this is optional.

We'll collect data on your task time, accuracy, and any preferences you provide. Your data will be linked to an anonymized user ID, with only your email stored for reward distribution.

## Potential Risks and Discomforts

The only known risk to participants of this study is in confidentiality of study participation. This risk will be mitigated through the procedures described in the section titled "Confidentiality" below.

## Potential Benefits

The benefits to participants include the ability to complete various tasks with model assistance, but there are no other direct benefits. However, this study aims to use techniques in machine learning and natural language processing to improve learning and the ability to complete tasks more efficiently. Participants in this study may therefore benefit from being able to learn how to better tackle tasks they are interested in improving in. Our user studies could also help participants prepare for specific competitions, such as writing and coding contests, through model-generated instructions and personalized guidance for task completion. Finally, our competition-based, gamified setting could provide enjoyment for participants, which is why we will add extra tasks to our website that participants could complete for fun.

## Confidentiality

Data collected in this study will be securely stored in a database hosted on password protected webservers and cloud-based file storage. Only researchers will have access to users' emails and no other identifying information will be collected in the app. After the end of the experiment, data will be anonymized and released publicly. The user's email will be kept in a separate table and linked to a username and user id. These user ids and usernames are used throughout the experiment to assign different models/instructions and for statistical leaderboards, but the collected usernames and emails and links to public statistics pages will be destroyed after the completion of this study. To maintain linkage in study data, a new randomized user id will be generated in replacement of the identification keys in use during the study.

If we write a report or article about this research project, your identity will be protected to the maximum extent possible. Your information may be shared with representatives of the University of Maryland, College Park or governmental authorities if you or someone else is in danger or if we are required to do so by law.

During the course of the study, the collected data will only be accessible to PI Jordan Boyd-Graber and Co-PIs Nishant Balepur, Shi Feng, and Matthew Shu. Your identifiable information (email address) will be deleted at the end of the study.

## Compensation

For each user study, we will pay recruited participants a fair, hourly wage (at a minimum $15/hour MD minimum wage) for completing tasks and skill checks. Users who perform exceptionally well in our competition and non-competition-based user studies can earn up to an extra $50 in additional rewards. We expect to distribute these extra rewards to a maximum of 10 participants. Participants will be eligible for extra compensation over the course of each planned time span for the user studies and will be distributed at most on a monthly basis. Participants will receive the base $15/hour rewards after we verify that they completed the instructions for our user study (ETA 1 week maximum), and the extra rewards within 1.5 months maximum of participating. The extra rewards will take slightly longer to distribute, as we must calculate who were the highest performers at the end of the user study duration (expected to be 1 month).

Participants who are recruited from specific university courses (listed below) will also have the opportunity to earn extra credit. These students will be able to choose just one of the extra credit or the monetary compensation as their reward. We will provide an alternative extra credit assignment that students can complete if they do not wish to participate in our study. We have currently discussed this research opportunity with the following instructors. We also list their courses and the expected amount of extra credit:

- **Rachel Rudinger (UMD)**, CMSC 470 - Introduction to Natural Language Processing: Minimum of +1% on the final exam
- **Fumeng Yang (UMD)**, CMSC471: Introduction to Information Visualization: Minimum of +1% in the class
- **Shi Feng (GWU)**, CSCI4364/6364 Machine Learning: Minimum of +1% in the class
- **Jordan Boyd-Graber (UMD)**: Minimum of +1% in the class
- **Eunsol Choi (NYU)**: Minimum of +1% in the class

## Right to Withdraw and Questions

Your participation in this research is completely voluntary. You may choose not to take part at all. If you decide to participate in this research, you may stop participating at any time. If you decide not to participate in this study or if you stop participating at any time, you will not be penalized or lose any benefits to which you otherwise qualify.

If you are recruited from university courses, the course instructor will not have access to student participants' PII such as email address. Participation in this study is completely voluntary and your decision to participate or not participate will not have any effect on your grades, reputation, or standing in the course.

If you decide to stop taking part in the study, if you have questions, concerns, or complaints, or if you need to report an injury related to the research, please contact the investigator:

**Jordan Boyd-Graber**  
4146 Iribe University of Maryland  
College Park, Maryland, 20742  
E-mail: jbg@umiacs.umd.edu  
Telephone: (301) 405-6766

## Participant Rights

If you have questions about your rights as a research participant or wish to report a research-related injury, please contact:

**University of Maryland College Park**  
Institutional Review Board Office  
1204 Marie Mount Hall  
College Park, Maryland, 20742  
E-mail: irb@umd.edu  
Telephone: 301-405-0678

For more information regarding participant rights, please visit:  
https://research.umd.edu/research-resources/research-compliance/institutional-review-board-irb/research-participants

This research has been reviewed according to the University of Maryland, College Park IRB procedures for research involving human subjects.

## Statement of Consent

By clicking "I agree", you indicate that you are at least 18 years of age; you have read this consent form or have had it read to you; your questions have been answered to your satisfaction and you voluntarily agree to participate in this research study. We advise you to download a copy of this consent form for your own records.`